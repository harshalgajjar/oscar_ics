{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e38f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from flask import Flask, request, render_template, url_for\n",
    "#from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc750ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__name__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with windowsapi reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "app = Flask(\"__name__\")\n",
    "\n",
    "q = \"\"\n",
    "\n",
    "@app.route(\"/\")\n",
    "def loadPage():\n",
    "    return render_template('sandy1.html', query=\"\")\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/abc\", methods=['POST'])\n",
    "def predict():\n",
    "    \n",
    "    url = './sand transport.csv'  # why\n",
    "    column_names = ['concentration', 'Vsl', 'Vl', 'Particle Size', 'Class' ]\n",
    "    sand = pd.read_csv(url, names=column_names, sep=',') \n",
    "    \n",
    "    \n",
    "    X = sand\n",
    "    Y = X.pop('Class')\n",
    "    \n",
    "    \n",
    "    from sklearn.model_selection import train_test_split  # shuffle and split it\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state=48)\n",
    "   \n",
    "\n",
    "    #Normlization\n",
    "    #from sklearn.preprocessing import Normalizer\n",
    "    #transformer = Normalizer().fit(X_train)                               ### here\n",
    "    #X_train_normalized=transformer.transform(X_train)\n",
    "\n",
    "\n",
    "    #Standarization\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_normalized = scaler.transform(X_train)\n",
    "    \n",
    "    \n",
    "    param_grid = {'C':[0.1,1,10,100, 150, 200,250,300, 350,400,450,500,550],'gamma':[4,3.5,3,2.5,2,1.5,1,0.1,0.01,0.001],'kernel':['rbf']}\n",
    "    from sklearn.svm import SVC\n",
    "    SVM_model = SVC(gamma='auto')\n",
    "    \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    rf_grid = RandomizedSearchCV (estimator = SVM_model, param_distributions = param_grid, cv = 3, verbose=2, n_jobs=4)\n",
    "    rf_grid.fit(X_train_normalized, y_train)\n",
    "    \n",
    "    \n",
    "    svclassifier = SVC(kernel='rbf',C=rf_grid.best_params_['C'],gamma=rf_grid.best_params_['gamma'])  # change gamma and C based on above hyperparameter optimization\n",
    "    svclassifier.fit(X_train_normalized, y_train)\n",
    "       \n",
    "       \n",
    "    inputQuery1 = request.form['query1']\n",
    "    inputQuery2 = request.form['query2']\n",
    "    inputQuery3 = request.form['query3']\n",
    "    inputQuery4 = request.form['query4']\n",
    "\n",
    "    \n",
    "    #Normalization\n",
    "    #tes2 = transformer.transform(np.array([[inputQuery1,inputQuery2,inputQuery3,inputQuery4],]))\n",
    "    #output = svclassifier.predict(tes2)\n",
    "    \n",
    "    #Standarization\n",
    "    tes2 = scaler.transform(np.array([[inputQuery1,inputQuery2,inputQuery3,inputQuery4],]))\n",
    "    output = svclassifier.predict(tes2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return render_template('sandy1.html', output=output, query1 = request.form['query1'], query2 = request.form['query2'],query3 = request.form['query3'],query4 = request.form['query4'])\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55059e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = './sand transport.csv'  # why\n",
    "column_names = ['concentration', 'Vsl', 'Vl', 'Particle Size', 'Class' ]\n",
    "sand = pd.read_csv(url, names=column_names, sep=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04bf9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sand\n",
    "Y = X.pop('Class')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # shuffle and split it\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60dacf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.00000000e+03, 1.00000000e-01, 6.89813771e-01, 1.88000000e-04],\n",
       "       [5.00000000e+02, 1.00000000e-01, 6.84753787e-01, 6.70000000e-05],\n",
       "       [6.00000000e+03, 5.00000000e-02, 8.35730630e-01, 5.12000000e-04],\n",
       "       ...,\n",
       "       [1.00000000e+04, 1.20000000e-01, 1.26000000e+00, 6.70000000e-05],\n",
       "       [2.50000000e+02, 5.00000000e-02, 8.11513779e-01, 1.88000000e-04],\n",
       "       [6.00000000e+03, 1.00000000e-01, 8.38287211e-01, 1.88000000e-04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e73c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normlization\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#transformer = Normalizer().fit(X_train)                               ### here\n",
    "#X_train_normalized=transformer.transform(X_train)\n",
    "\n",
    "\n",
    "#Standarization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd78610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C':[0.1,1,10,100, 150, 200,250,300, 350,400,450,500,550],'gamma':[4,3.5,3,2.5,2,1.5,1,0.1,0.01,0.001],'kernel':['rbf']}\n",
    "from sklearn.svm import SVC\n",
    "SVM_model = SVC(gamma='auto')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_grid = RandomizedSearchCV (estimator = SVM_model, param_distributions = param_grid, cv = 3, verbose=2, n_jobs=4)\n",
    "rf_grid.fit(X_train_normalized, y_train)\n",
    "\n",
    "\n",
    "svclassifier = SVC(kernel='rbf',C=rf_grid.best_params_['C'],gamma=rf_grid.best_params_['gamma'])  # change gamma and C based on above hyperparameter optimization\n",
    "svclassifier.fit(X_train_normalized, y_train)\n",
    "\n",
    "\n",
    "inputQuery1 = 1\n",
    "inputQuery2 = 1\n",
    "inputQuery3 = 1\n",
    "inputQuery4 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1377b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes2 = scaler.transform(np.array([[inputQuery1,inputQuery2,inputQuery3,inputQuery4],]))\n",
    "output = svclassifier.predict(tes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a7fa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Normalization\n",
    "#tes2 = transformer.transform(np.array([[inputQuery1,inputQuery2,inputQuery3,inputQuery4],]))\n",
    "#output = svclassifier.predict(tes2)\n",
    "\n",
    "#Standarization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "return render_template('sandy1.html', output=output, query1 = request.form['query1'], query2 = request.form['query2'],query3 = request.form['query3'],query4 = request.form['query4'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
